# -*- coding: utf-8 -*-
"""Recipe Recommendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tXykagMUZgMTMz2UafaxsCFT0BiXb1wi
"""

import nltk
nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger_eng')

!pip install gensim
import gensim.downloader as api
import numpy as np
from sklearn.cluster import KMeans
from collections import defaultdict

# Library
import pandas as pd
import numpy as np
import re
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string
# import kagglehub

# Download latest version
# path = kagglehub.dataset_download("trainingdatapro/ocr-receipts-text-detection")

# print("Path to dataset files:", path)
# !pip install pytesseract

from PIL import Image
# import pytesseract
from nltk.util import ngrams
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt_tab')

# import data
recipe = pd.read_json ("./recipes_raw_nosource_ar.json").T
# recipe = recipe.reset_json()

# cleaning data
recipe.reset_index(drop=True, inplace=True)
recipe.dropna(inplace=True)
recipe.head()

# test = ['1/2 pound Brussels sprouts, end trimmed and yellow leaf removed', '1/2 cup all-purpose flour']
# test = [ '1/4 cup water',
#  '1/2 pound pot roast']
# # l  =re.search(r"([\d\/]*\d+)\s*(pound[s]*|cup[s]*|tablespoon[s]*|teaspoon[s]*|\(.*\)\s\w*)*\s*(.*)",test[0] )
# # print ("q: ",l.group(1),"\nu: ",l.group(2),"\ning: ",l.group(3))

# l = re.search(r"([\d\/]*\d+) (cup[s]*|tablespoon[s]*|teaspoon[s]*|\(.*\)\s\w*)*(.*)",test[1] )
# print ("q: ",l.group(1),"\nu: ",l.group(2),"\ning:",l.group(3))

corpus = []
def text_preprocess(text):
  """Removes punctuation from a text string."""
  lemmatizer = WordNetLemmatizer()
  # Create a translation table that maps all punctuation characters to None
  translator = str.maketrans('', '', string.punctuation)
  text = text.translate(translator)
  text = [lemmatizer.lemmatize(word) for word in text.lower().split()]
  # Use the translate method to remove punctuation
  return " ".join(text)

def convertingIngredients (x):
  output = []

  for i in range(len(x)):
    ing = re.search(r"(\d*\s*[\d\/]*\d+)\s*(ounce[s]*|pound[s]*|cup[s]*|tablespoon[s]*|teaspoon[s]*|\(.*\)\s\w*)*\s*(.*)",x[i].replace("ADVERTISEMENT","").strip() )
    if ing:
      corpus.append (text_preprocess(ing.group(3)) )
      output.append({"quantity": ing.group(1).strip(),"unit": ing.group(2), "ingredient": ing.group(3)})
  return output
recipe['tag'] = recipe.ingredients.apply(convertingIngredients)
recipe.head()

# Commented out IPython magic to ensure Python compatibility.
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline
word_cloud1 = WordCloud(collocations = False, background_color = 'white',
                        width = 2048, height = 1080).generate(" ".join(corpus))
plt.imshow(word_cloud1, interpolation='bilinear')
plt.axis("off")
plt.show()

methods = ['shredded','sliced','minced','chopped','diced','cubed','finely','pinched','skinless','softened','melted','thinly']
type_ = ['powder','dried','fresh','whole','ground','or more to taste']

from nltk.probability import FreqDist
import matplotlib.pyplot as plt

fdist = FreqDist(" ".join(corpus).split())
top_words = fdist.most_common(20)

words, frequencies = zip(*top_words)

plt.figure(figsize=(10, 6))
plt.barh(words, frequencies[::-1])
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.title("Top 20 Most Frequent Words in Corpus")
plt.yticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

from nltk.probability import FreqDist
import matplotlib.pyplot as plt

fdist = FreqDist(corpus)
top_words = fdist.most_common(20)

words, frequencies = zip(*top_words)

plt.figure(figsize=(10, 6))
plt.barh(words, frequencies[::-1])
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.title("Top 20 Iingredients")
plt.yticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# --- Step 1: Load a pre-trained GloVe word embedding model ---
# This will download a model if you don't have it already.
# We use a small 50-dimensional model for a quick demonstration.
print("Downloading GloVe model...")
try:
    glove_model = api.load("glove-wiki-gigaword-50")
    print("Model loaded successfully.")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Please check your internet connection or install the model manually.")
    exit()


def get_phrase_vector(phrase, model):
    """
    Computes a vector for a phrase by averaging its word vectors.
    Handles words not found in the model's vocabulary.
    """
    words = phrase.lower().split()
    valid_vectors = [model[word] for word in words if word in model]

    if not valid_vectors:
        return None # Return None if no words are in the vocabulary

    # Average the vectors to get a single phrase vector
    return np.mean(valid_vectors, axis=0)

# add pos tags and remove stop words and weight the embedding according to that

# import nltk
# from nltk.corpus import stopwords
# from nltk.tag import pos_tag

# # Ensure necessary NLTK data is downloaded
# nltk.download('averaged_perceptron_tagger')
# nltk.download('stopwords')

# stop_words = set(stopwords.words('english'))
# methods = ['shredded','sliced','minced','chopped','diced','cubed','finely','pinched','skinless','softened','melted','thinly','grated']
# type_ = ['powder','dried','fresh','whole','ground','or more to taste','bunch']
# # Combine the lists to remove
# remove_words = set(methods + type_ + list(stop_words))

# def get_weighted_phrase_vector(phrase, model):
#     """
#     Computes a weighted vector for a phrase by averaging its word vectors.
#     Weights are based on POS tags, and stop words/specific terms are removed.
#     Handles words not found in the model's vocabulary.
#     """
#     words = word_tokenize(phrase.lower())
#     tagged_words = pos_tag(words)

#     valid_vectors = []
#     weights = []

#     for word, tag in tagged_words:
#         if word in model and word not in remove_words:
#             # Assign higher weight to nouns (NN, NNS, NNP, NNPS)
#             if tag.startswith('NN'):
#                 weight = 2.0 # increase
#             else:
#                 weight = 0.5  # Lower weight for other POS tags

#             valid_vectors.append(model[word])
#             weights.append(weight)

#     if not valid_vectors:
#         return None

#     # Apply weights to vectors and average
#     weighted_vectors = [vec * w for vec, w in zip(valid_vectors, weights)]
#     return np.mean(weighted_vectors, axis=0)

# # Create weighted phrase vectors for the corpus
# weighted_phrase_vectors = []
# valid_weighted_phrases = []

# # Assuming 'corpus' is a list of phrases from previous steps
# for phrase in corpus[:1000]:
#     vec = get_weighted_phrase_vector(phrase, glove_model)
#     if vec is not None:
#         weighted_phrase_vectors.append(vec)
#         valid_weighted_phrases.append(phrase)

# # You can now use 'weighted_phrase_vectors' for clustering or other tasks
# # For example, applying KMeans clustering:
# num_clusters = 700 # Or your desired number of clusters
# kmeans_weighted = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
# kmeans_weighted.fit(weighted_phrase_vectors) # change
# weighted_cluster_labels = kmeans_weighted.labels_

# # Group phrases by weighted cluster labels (similar to the previous step)
# weighted_clusters = defaultdict(list)
# for phrase, label in zip(valid_weighted_phrases, weighted_cluster_labels):
#     weighted_clusters[label].append(phrase)

# # print("\n--- Weighted Phrase Clusters ---")
# # for label, phrase_list in weighted_clusters.items():
# #     print(f"\nCluster {label}:")
# #     for phrase in phrase_list:
# #         print(f"  - {phrase}")



# import joblib

# # Assuming kmeans_weighted has been trained in a previous cell
# # Save the model to a file
# filename = 'kmeans_weighted_model.joblib'
# joblib.dump(kmeans_weighted, filename)

# print(f"Model saved to {filename}")

import joblib

# Replace 'your_uploaded_model.joblib' with the actual name of your uploaded file
loaded_model = joblib.load('kmeans_weighted_model.joblib')

print("Model loaded successfully!")
# You can now use the loaded_model to make predictions or further analysis

clusters[8]



